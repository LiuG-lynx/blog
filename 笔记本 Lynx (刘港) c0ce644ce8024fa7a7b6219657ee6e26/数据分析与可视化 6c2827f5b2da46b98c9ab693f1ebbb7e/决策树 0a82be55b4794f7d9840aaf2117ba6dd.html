<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>决策树</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="0a82be55-b479-4f7d-9840-aaf2117ba6dd" class="page sans"><header><img class="page-cover-image" src="https://www.notion.so/images/page-cover/woodcuts_14.jpg" style="object-position:center 60%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">🌁</span></div><h1 class="page-title">决策树</h1></header><div class="page-body"><figure id="c81ff572-80e3-4f71-b61a-0f978dbec05b"><a href="https://sklearn.apachecn.org/docs/0.21.3/11.html#1106-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95--id3,-c45,-c50-%E5%92%8C-cart" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">1.10. 决策树</div><div class="bookmark-description">校验者: @文谊 @皮卡乒的皮卡乓 @Loopy 翻译者:@I Remember Decision Trees (DTs) 是一种用来 classification 和 regression 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。 例如，在下面的图片中，决策树通过if-then-else的决策规则来学习数据从而估测数一个正弦图像。决策树越深入，决策规则就越复杂并且对数据的拟合越好。 决策树的优势: 决策树的缺点包括: 决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现 该问题最为有效地方法。 决策树可能是不稳定的，因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解 在多方面性能最优和简单化概念的要求下，学习一棵最优决策树通常是一个NP难问题。因此，实际的决策树学习算法是基于启发式算法，例如在每个节点进 行局部最优决策的贪心算法。这样的算法不能保证返回全局最优决策树。这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。 有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。 如果某些类在问题中占主导地位会使得创建的决策树有偏差。因此，我们建议在拟合前先对数据集进行平衡。 DecisionTreeClassifier 是能够在数据集上执行多分类的类,与其他分类器一样，DecisionTreeClassifier 采用输入两个数组：数组X，用 [n_samples, n_features] 的方式来存放训练样本。整数值数组Y，用 [n_samples] 来保存训练样本的类标签: from sklearn import tree X = [[0, 0], [1, 1]] Y = [0, 1] clf = tree.DecisionTreeClassifier() clf = clf.fit(X, Y) 执行通过之后，可以使用该模型来预测样本类别: clf.predict([[2., 2.]]) array([1]) 另外，也可以预测每个类的概率，这个概率是叶中相同类的训练样本的分数: clf.predict_proba([[2., 2.]]) array([[ 0., 1.]]) DecisionTreeClassifier 既能用于二分类（其中标签为[-1,1]）也能用于多分类（其中标签为[0,...,k-1]）。使用Lris数据集，我们可以构造一个决策树，如下所示: from sklearn.datasets import load_iris from sklearn import tree iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) 经过训练，我们可以使用 export_graphviz 导出器以 Graphviz 格式导出决策树.</div></div><div class="bookmark-href"><img src="https://sklearn.apachecn.org/docs/0.21.3/gitbook/images/favicon.ico" class="icon bookmark-icon"/>https://sklearn.apachecn.org/docs/0.21.3/11.html#1106-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95--id3,-c45,-c50-%E5%92%8C-cart</div></div></a></figure><p id="8c4020eb-fcff-467f-954d-534fd1b5ad6b" class=""><strong>构造</strong></p><p id="d1ebf27d-8299-48db-9be4-6fb210f90c2a" class="">构造的过程就是选择什么属性作为节点的过程</p><p id="5162cdb0-9b3e-483b-9e3a-3a287b0b8cb4" class=""><strong>剪枝</strong></p><p id="1ee54fa5-ef48-4177-b626-4cfb1d2b1f97" class="">过拟合 Overfitting </p><p id="d9cce569-efdd-4d54-8dc5-e907de00a40d" class="">模型训练的太好过于理想化</p><p id="6827f0ba-433c-49a2-8dde-1637627ed719" class="">
</p><p id="066c7992-eabc-4018-9b5c-017fd67d0027" class="">剪枝可以分为“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）。</p><p id="3b7a962a-09f0-4b6c-b686-d260ea973c45" class=""><code>预剪枝</code>是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。</p><p id="b50f37c6-973f-4d2f-8e05-78cc34f5aba9" class=""><code>后剪枝</code>就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</p><p id="2d4babed-dee4-4ada-88c5-aabec717317f" class=""><strong>在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。</strong></p><p id="5cfbaf23-cd94-4264-808f-67dd324ad313" class=""><strong>在属性选择的标准上，度量方法包括了信息增益和信息增益率。</strong></p><p id="59318474-28b9-4ba5-b366-b31a23e7fbd5" class=""><strong>纯度和信息熵。</strong></p><p id="f2f7847c-3659-4af5-b310-964ccb767542" class=""><code>纯度</code>你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。</p><p id="1290dee5-837e-45d6-8c9e-64e6d5377994" class=""><code>信息熵</code> entropy 代表信息的不确定性</p><p id="df676c73-8d34-43b4-8544-ee2ed6ef4eb0" class="">信息熵越大, 纯度越低</p><p id="63b3900a-40f1-47b7-a156-336f33ace80c" class="">所有样本均匀混合 信息熵最大,纯度最低</p><p id="5b32c943-9ef9-4c22-87f5-99f15734f9b1" class="">“不纯度”的指标有三种</p><p id="15518076-eedf-491d-9d62-986539be4989" class="">分别是信息增益（ID3算法）、信息增益率（C4.5算法）以及基尼指数（Cart算法）。</p><p id="c960288d-9d62-4bf3-8248-b6303be14569" class=""><strong>ID3算法计算的是信息增益</strong></p><p id="13e2e430-621a-4dc7-9cea-b46b53f96c55" class="">信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。</p><p id="b5a82702-4532-4453-b117-926a23326687" class="">有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。</p><p id="cdfbc777-bcbc-4f77-b95b-0c8b972c1e29" class=""><strong>在ID3算法上进行改进的C4.5算法</strong></p><p id="abfbb0e6-b37d-465d-a85e-c32d50fa8a67" class=""><strong>1. 采用信息增益率</strong></p><p id="ff0bbfcf-0958-47e9-862b-2d3f7f644917" class="">信息增益率=信息增益/属性熵</p><p id="8b50a07e-6a58-4148-9074-95ae0b346b9c" class=""><strong>2. 采用悲观剪枝</strong></p><p id="511ab3dd-d3ba-4d38-b5c8-73076eba31fc" class="">ID3构造决策树的时候，容易产生过拟合的情况。在C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。</p><p id="217d02e4-8a9b-4374-81f3-518ed44458fe" class="">悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</p><p id="ad98a64d-c801-4775-b5e0-3343c4ca93a4" class=""><strong>3. 离散化处理连续属性</strong></p><p id="1c6bfb6c-4e23-4852-aed3-6cda7b3ef30f" class="">C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，<strong>C4.5选择具有最高信息增益的划分所对应的阈值</strong>。</p><p id="8fc8717c-2f16-4f4b-b0c5-99d744a59412" class=""><strong>4. 处理缺失值</strong></p><h2 id="88d78f61-8b42-41bc-acf0-845cf133284b" class="">CART 算法</h2><p id="c578448e-6ef8-468e-a57f-87cab8fedf54" class="">CART算法，英文全称叫做Classification And Regression Tree，中文叫做分类回归树。</p><p id="e454dd0a-6928-4a3b-80c6-96e309d0109d" class="">ID3和C4.5算法可以生成二叉树或多叉树，而CART只支持二叉树。同时CART决策树比较特殊，既可以作分类树，又可以作回归树。</p><h3 id="47f42dbe-58bd-4d10-a0f6-ed879a1d4153" class="">分类树</h3><p id="a2a78e23-ba81-4c11-948f-fd558c7991d6" class="">分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别</p><p id="4c5d7ac6-1052-4543-aaa6-4ba8aee5fdc0" class="">分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。</p><p id="9e90ba42-68ac-4c29-a9b6-2af1b1b8a0c0" class="">基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。</p><p id="80068a6d-270b-4567-892d-2cbbec19fb92" class="">
</p><p id="f550ce12-9068-448d-96c9-d71509f06c04" class="">假设t为节点，那么该节点的GINI系数的计算公式为：</p><figure id="16176e2d-6cf1-4cfb-9078-a045b3644225" class="image"><a href="https://static001.geekbang.org/resource/image/f9/89/f9bb4cce5b895499cabc714eb372b089.png"><img src="https://static001.geekbang.org/resource/image/f9/89/f9bb4cce5b895499cabc714eb372b089.png"/></a></figure><p id="f794babf-9bb5-4f16-aa34-01f8bd98bc93" class="">这里p(Ck|t)表示节点t属于类别Ck的概率，节点t的基尼系数为1减去各类别Ck概率平方和。</p><p id="e9a7b8d4-796b-448c-a7c6-8df3fcede5a1" class="">通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：</p><p id="bfbe96ed-0dc9-4801-87f2-64eb6537daad" class="">集合1：6个都去打篮球；</p><p id="eb86b811-522f-4cfe-abd2-43768ff23261" class="">集合2：3个去打篮球，3个不去打篮球。</p><p id="28c7c87a-e245-45b4-9099-8cc115764cb5" class="">针对集合1，所有人都去打篮球，所以p(Ck|t)=1，因此GINI(t)=1-1=0。</p><p id="739c6400-cb0b-472f-87d5-9efd72a340a4" class="">针对集合2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5*0.5+0.5*0.5）=0.5。</p><p id="26dd608a-09e1-489d-8815-64982836950f" class="">通过两个基尼系数你可以看出，集合1的基尼系数最小，也证明样本最稳定，而集合2的样本不稳定性更大。</p><p id="c6bafc2b-d513-46a3-8841-19b85fcc3b04" class="">以CART算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。</p><p id="cee1ff44-0037-44a2-84cd-c25e45f6d16d" class=""><strong>使用CART算法来创建分类树</strong></p><pre id="e54ee824-2523-4487-91ef-f857c90777c7" class="code"><code># encoding=utf-8
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
# 准备数据集
iris=load_iris()
# 获取特征集和分类标识
features = iris.data
labels = iris.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)
# 创建CART分类树
clf = DecisionTreeClassifier(criterion=&#x27;gini&#x27;)
# 拟合构造CART分类树
clf = clf.fit(train_features, train_labels)
# 用CART分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print(&quot;CART分类树准确率 %.4lf&quot; % score)</code></pre><p id="8ece52b4-78a2-4377-b423-3c6aed87352f" class="">运行结果：</p><pre id="0ae92df0-dc75-401a-8e99-3c1deea35fd8" class="code"><code>CART分类树准确率 0.9600</code></pre><p id="7d0b9b35-7290-4dfa-b16c-f000e6cb1b75" class="">如果我们把决策树画出来，可以得到下面的图示：</p><figure id="841ac478-a7a0-4e3b-a4f7-3ff38a071209" class="image"><a href="https://static001.geekbang.org/resource/image/c1/40/c1e2f9e4a299789bb6cc23afc6fd3140.png"><img src="https://static001.geekbang.org/resource/image/c1/40/c1e2f9e4a299789bb6cc23afc6fd3140.png"/></a></figure><p id="e69e74aa-3e89-4771-92d5-fb2453164d3c" class="">首先train_test_split可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。</p><p id="659a955d-11dc-474e-a6c9-195d6356eba9" class="">使用clf = DecisionTreeClassifier(criterion=‘gini’)初始化一棵CART分类树。这样你就可以对CART分类树进行训练。</p><p id="11f56814-ae63-4f84-aa00-937ef68f3541" class="">使用clf.fit(train_features, train_labels)函数，将训练集的特征值和分类标识作为参数进行拟合，得到CART分类树。</p><p id="f1e1c359-4735-4845-b327-71769be7744c" class="">使用clf.predict(test_features)函数进行预测，传入测试集的特征值，可以得到测试结果test_predict。</p><p id="a823b912-5823-4205-9222-7dcce170d8c4" class="">最后使用accuracy_score(test_labels, test_predict)函数，传入测试集的预测结果与实际的结果作为参数，得到准确率score。</p><p id="c22c98fe-9c1d-451d-9a52-6dd34a7d9c8a" class=""><strong>回归树</strong></p><p id="a16a8f93-d316-4873-9fc2-7dc281dc4dc9" class="">回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p><p id="7ae78d39-ec9e-44d7-8035-2d8580ba950f" class="">回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在CART分类树中采用的是基尼系数作为标准，那么在CART回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。</p><p id="da034e49-5a11-4558-ab19-fd32a32ad8bf" class="">样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设x为样本的个体，均值为u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p><p id="52649280-6540-4eca-82c3-c31041275876" class="">其中差值的绝对值为样本值减去样本均值的绝对值：</p><figure id="729dacbb-a6ff-4554-a652-b995f00b38d3" class="image"><a href="https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png"><img src="https://static001.geekbang.org/resource/image/6f/97/6f9677a70b1edff85e9e467f3e52bd97.png"/></a></figure><p id="5f19682e-2b4e-4544-a2ae-7e02902dc2e0" class="">方差为每个样本值减去样本均值的平方和除以样本个数：</p><figure id="43b331cd-0dc0-4840-a722-734f0fbb5856" class="image"><a href="https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png"><img src="https://static001.geekbang.org/resource/image/04/c1/045fd5afb7b53f17a8accd6f337f63c1.png"/></a></figure><p id="ae9d8b52-a56a-4893-a6ae-c5eabd3caa59" class="">所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p><p id="037c4392-ab1c-4c24-986e-952e60582899" class="">
</p><p id="878ffc89-b32b-4e05-8111-43d80fd908f4" class="">根据这些指标，我们使用CART回归树对波士顿房价进行预测，代码如下：</p><pre id="4608238e-b59f-4ba7-8f6d-304174a296e3" class="code"><code># encoding=utf-8
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
from sklearn.tree import DecisionTreeRegressor
# 准备数据集
boston=load_boston()
# 探索数据
print(boston.feature_names)
# 获取特征集和房价
features = boston.data
prices = boston.target
# 随机抽取33%的数据作为测试集，其余为训练集
train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)
# 创建CART回归树
dtr=DecisionTreeRegressor()
# 拟合构造CART回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print(&#x27;回归树二乘偏差均值:&#x27;, mean_squared_error(test_price, predict_price))
print(&#x27;回归树绝对值偏差均值:&#x27;, mean_absolute_error(test_price, predict_price))</code></pre><p id="cd8ac0c0-096d-438a-a5c1-afa9eeb6209a" class="">运行结果（每次运行结果可能会有不同）：</p><pre id="9e3f4b67-f31a-488a-a29c-8b55c61d1a68" class="code"><code>[&#x27;CRIM&#x27; &#x27;ZN&#x27; &#x27;INDUS&#x27; &#x27;CHAS&#x27; &#x27;NOX&#x27; &#x27;RM&#x27; &#x27;AGE&#x27; &#x27;DIS&#x27; &#x27;RAD&#x27; &#x27;TAX&#x27; &#x27;PTRATIO&#x27; &#x27;B&#x27; &#x27;LSTAT&#x27;]
回归树二乘偏差均值: 23.80784431137724
回归树绝对值偏差均值: 3.040119760479042</code></pre><p id="3e915ee2-11e1-4adb-adf7-04bdf5d315a0" class="">如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：</p><figure id="75848de8-df18-4d56-9c51-3eabd7c81882" class="image"><a href="https://static001.geekbang.org/resource/image/65/61/65a3855aed648b32994b808296a40b61.png"><img src="https://static001.geekbang.org/resource/image/65/61/65a3855aed648b32994b808296a40b61.png"/></a></figure><p id="70943ecc-f528-4851-b94c-c868b6087380" class="">你可以在<a href="https://pan.baidu.com/s/1RKD6-IwAzL--cL0jt4GPiQ">这里</a>下载完整PDF文件。</p><p id="2a9702c9-d48b-4005-80a8-2e6c6dfa33e1" class="">我们来看下这个例子，首先加载了波士顿房价数据集，得到特征集和房价。然后通过train_test_split帮助我们把数据集抽取一部分作为测试集，其余作为训练集。</p><p id="b3f837e0-2a47-4fad-976c-636a48984897" class="">使用dtr=DecisionTreeRegressor()初始化一棵CART回归树。</p><p id="e7d75ac6-5054-48cd-a313-210b286930db" class="">使用dtr.fit(train_features, train_price)函数，将训练集的特征值和结果作为参数进行拟合，得到CART回归树。</p><p id="e3a2cf41-0f2b-4856-9487-0d5f0903374a" class="">使用dtr.predict(test_features)函数进行预测，传入测试集的特征值，可以得到预测结果predict_price。</p><p id="58bb5e30-8cb1-4ec8-a2d7-454165648293" class="">最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。</p><p id="04e99785-337e-44c3-a8b5-6b46aaf9782e" class="">我们能看到CART回归树的使用和分类树类似，只是最后求得的预测值是个连续值。</p><h2 id="2fa27da0-c9be-4636-812f-b9a90e136921" class=""><strong>CART决策树的剪枝</strong></h2><p id="4a07e80f-a1e8-437a-8745-2e7a0dca199d" class="">CART决策树的剪枝主要采用的是CCP方法，它是一种后剪枝的方法，英文全称叫做cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：</p><figure id="af982d6c-b1ba-49df-b6ff-b24f742869cb" class="image"><a href="https://static001.geekbang.org/resource/image/6b/95/6b9735123d45e58f0b0afc7c3f68cd95.png"><img src="https://static001.geekbang.org/resource/image/6b/95/6b9735123d45e58f0b0afc7c3f68cd95.png"/></a></figure><p id="2ea61d29-b59b-40c9-8d96-7ca91900eb94" class="">其中Tt代表以t为根节点的子树，C(Tt)表示节点t的子树没被裁剪时子树Tt的误差，C(t)表示节点t的子树被剪枝后节点t的误差，|Tt|代子树Tt的叶子数，剪枝后，T的叶子数减少了|Tt|-1。</p><p id="db641e9e-0cf3-4a4f-8b29-2254c709c387" class="">所以节点的表面误差率增益值等于节点t的子树被剪枝后的误差变化除以剪掉的叶子数量。</p><p id="50b7bc2e-94b4-470b-8afd-1e1ab079f0d3" class="">因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。</p><p id="68d38ac1-6ed4-454a-99be-13c08b069c32" class="">得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。</p><blockquote id="54d2b677-2c66-4d94-a6b3-bd6c019ff19b" class="">CART决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。</blockquote><p id="b8594f98-28a2-4dee-9b3a-2c6d6978e25b" class="">
</p><figure id="50d3938e-232a-48d9-a4f5-3d147bf88a7f" class="image"><a href="%E5%86%B3%E7%AD%96%E6%A0%91%200a82be55b4794f7d9840aaf2117ba6dd/Untitled.png"><img style="width:820px" src="%E5%86%B3%E7%AD%96%E6%A0%91%200a82be55b4794f7d9840aaf2117ba6dd/Untitled.png"/></a></figure><p id="b40d3cd5-d8ad-4003-b513-df9803ed51c8" class="">
</p></div></article></body></html>